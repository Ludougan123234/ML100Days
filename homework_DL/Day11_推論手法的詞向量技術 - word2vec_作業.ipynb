{"cells":[{"cell_type":"markdown","metadata":{"id":"qfsA2dhCy5Re"},"source":["### 作業目的: 了解為何需要推論方法的詞向量及其優缺點\n","\n","本次作業主要為思考題，請學員根據題目思考適合的回答\n","\n","<font color = 'green'>Objective: Understanding why inferential word embeddings are needed</font>"]},{"cell_type":"markdown","metadata":{"id":"FavhfM8dy5Ri"},"source":["### Q1:\n","請學員思考為何不直接使用one-hot encoding的方式建立詞向量，而要有其他計數方法與推論方法的詞向量產生方式？\n","\n","<font color='green'>Explain the rationale behind using other counting and inferential methods for generating word-embedding rather than just using one-hot encoding.</font>"]},{"cell_type":"markdown","metadata":{"id":"9fn6IUAXy5Rj"},"source":["### Answer:\n","- Curse of dimensionality - Considering that natural language is a type of data that has relatively high cardinality, one-hot encoding (OHE) generates representation that grows with the size of the corpus, and thus the dimension of the representation would be too high. Also, one-hot encoding should be used when the data is categorical but not ordinal (e.g., `[cat, dog, monkey]` is categorical but not ordinal while `[primary school, junior hs, senior hs]` could be considered categorical and ordinal) \n","\n","- Waste of storage - Extending on the first point, the nature of OHE can waste a lot of space for data with lots of unique values. \n","\n","- Distributed representation of the text could capture more information about a word. \n","\n","References: \n","1. [Quora Answer by Ajit Rajasekharan](https://www.quora.com/What-are-the-main-issues-with-using-one-hot-encoding)\n","2. [Data Science Stackexchange answer by AN6U5](https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor#:~:text=One%2DHot%2DEncoding%20has%20the,with%20the%20curse%20of%20dimensionality.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zT7DpUKNy5Rj"},"source":["### Q2:\n","相較於計數手法的詞向量(ex: one-hot, 共現矩陣, PPMI)，word2vec的方法有何優點？\n","\n","<font color='green'>What are the advantages of word2vec? (as compared to counting-based methods like OHE, co-occurence matrix, and PPMI)</font>"]},{"cell_type":"markdown","metadata":{"id":"mNR-3JRCy5Rk"},"source":["### Answer:\n","- Lower computational cost - When using counting-based methods, the entire word-embedding needs to be re-generated upon changes in corpus and would require a lot of time to compute if the corpus is too large. By contrast, word2vec resembles a neural network and uses small batches of data to train the network. Also, it is possible to speed up the training of a word2vec model by utilizing GPU resources. \n","\n","- Generalizability - The vector generated by word2vec is generalizable (semantically similar words have similar vectors), while counting-based method mostly only applies to the input corpus itself. \n","\n","Reference: \n","\n","1. Course material "]}],"metadata":{"kernelspec":{"display_name":"cupoy_env","language":"python","name":"cupoy_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Day11_推論手法的詞向量技術 - word2vec_作業.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}