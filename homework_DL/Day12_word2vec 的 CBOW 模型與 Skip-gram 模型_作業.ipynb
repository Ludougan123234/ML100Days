{"cells":[{"cell_type":"markdown","metadata":{"id":"TnW7p8oR--nr"},"source":["### 作業目的: 了解為何需要推論方法的詞向量及其優缺點\n","本次作業主要為思考題，請學員根據題目思考適合的回答"]},{"cell_type":"markdown","metadata":{"id":"Fi3bQ67K--nu"},"source":["### Question:\n","本次課程學習了word2vec中的兩個模型CBOW與Skip-gram，請學員思考與比較此兩種模型的優缺點?\n","\n","<font color='green'>Objective: Compare the advantages/disadvantages of CBOW and Skip-gram</font>"]},{"cell_type":"markdown","source":["### What is CBOW? (Reference: [CS 224D: Deep Learning for NLP Lecture Notes](http://cs224d.stanford.edu/lecture_notes/notes1.pdf))\n","- CBOW is a technique that is used to predict the center word using a given context\n","\n","- CBOW working mechanism: \n","   * Step 1: Generate a one-hot word vector $(x^{(c-m)},\\ldots,x^{(c-1)},x^{(c+1)},\\ldots,x^{(c+m)})$ for input size of m.\n","   * Step 2: Get the word-embedding of the one-hot word vector $\\left(v_{c-m} = \\mathcal{V} x^{(c-m)}, v_{c-m+1}=\\mathcal{V} x^{(c-m+1)}, \\ldots, v_{c+m}=\\mathcal{V} x^{(c+m)}\\right)$\n","   * Step 3: Average the vector to get $\\hat{v} = \\frac{v_{c-m} + v_{c-m+1}+\\cdots + v_{c+m}}{2m}$\n","   * Step 4: Generate the score vector $z = \\mathcal{U}\\hat{v}$\n","   * Step 5: Turn the scores into probabilities $\\hat{y} = \\text{softmax}(z)$\n","   * Step 6: Expect the probability of $\\hat{y}$ to match $y$, which is the one hot vector of the actual word \n","\n","   * Loss function (cross-entropy): $H(\\hat{y}, y) = - \\sum^{|V|}_{j=1}y_j\\log(\\hat{y}_j)$\n","\n","   * Optimization objective: \\begin{aligned}\n","\\operatorname{minimize} J &=-\\log P\\left(w_{c} \\mid w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m}\\right) \\\\\n","&=-\\log P\\left(u_{c} \\mid \\hat{v}\\right) \\\\\n","&=-\\log \\frac{\\exp \\left(u_{c}^{T} \\hat{v}\\right)}{\\sum_{j=1}^{|V|} \\exp \\left(u_{j}^{T} \\hat{v}\\right)} \\\\\n","&=-u_{c}^{T} \\hat{v}+\\log \\sum_{j=1}^{|V|} \\exp \\left(u_{j}^{T} \\hat{v}\\right)\n","\\end{aligned}"],"metadata":{"id":"FSvr-coGaAUB"}},{"cell_type":"markdown","source":["### What is Skip-gram? (Reference: [CS 224D: Deep Learning for NLP Lecture Notes](http://cs224d.stanford.edu/lecture_notes/notes1.pdf))\n","- Skip-gram is a technique that is used to predict the context using a given center word\n","\n","- CBOW working mechanism: \n","   * Step 1: Generate a one-hot word vector $x$.\n","   * Step 2: Get the word-embedding of the one-hot word vector $\\left(v_{c} = \\mathcal{V}_x\\right)$\n","   * Step 3: Simply set $\\hat{v} = v_c$\n","   * Step 4: Generate 2m score vectors $\\left(v_{c-m},\\ldots,v_{c-1}, v_{c+1},\\ldots, v_{c+m}\\right)$ using $u = \\mathcal{U}Vc$\n","   * Step 5: Turn the scores into probabilities $y = \\text{softmax}(u)$\n","   * Step 6: Expect the probability to match the one hot vector of the actual word \n","\n","\n","   * Optimization objective (using Naive-Bayes): \\begin{aligned}\n","\\operatorname{minimize} J &=-\\log P\\left(w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m} \\mid w_{c}\\right) \\\\\n","&=-\\log \\prod_{j=0, j \\neq m}^{2 m} P\\left(w_{c-m+j} \\mid w_{c}\\right) \\\\\n","&=-\\log \\prod_{j=0, j \\neq m}^{2 m} P\\left(u_{c-m+j} \\mid v_{c}\\right) \\\\\n","&=-\\log \\prod_{j=0, j \\neq m}^{2 m} \\frac{\\exp \\left(u_{c-m+j}^{T} v_{c}\\right)}{\\sum_{k=1}^{|V|} \\exp \\left(u_{k}^{T} v_{c}\\right)} \\\\\n","&=-\\sum_{j=0, j \\neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \\log \\sum_{k=1}^{|V|} \\exp \\left(u_{k}^{T} v_{c}\\right)\n","\\end{aligned}\n"],"metadata":{"id":"Kwqt7MK4EM_e"}},{"cell_type":"markdown","metadata":{"id":"tKgTmXy8--nv"},"source":["### Answers to the question:\n","\n","* **CBOW**:\n","  * Advantages: \n","    1. More efficient training\n","  * Disadvantages: \n","    1. Cannot deal with polysemy (word with different meanings - doing so would require a sense embedding) \n","    2. Fails to identify combined word phrases \n","    3. Rare words will be smoothed over by frequent words. \n","\n","* **Skip-gram**:\n","  * Advantages: \n","      1. More reliable results, especially for infrequent words\n","    * Disadvantages: \n","      1. Longer train time.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCvoMvu7--nw"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Day12_word2vec 的 CBOW 模型與 Skip-gram 模型_作業.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}