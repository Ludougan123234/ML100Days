{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day36_Sentiment analysis.ipynb","provenance":[],"authorship_tag":"ABX9TyMZH9nGMU3SyA4P8eFAP8/P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WgSoxfPy1XRa"},"source":["# 題目:電商產品評分文件以機器學習方式分辨是否為正向或負向\n","\n","說明：輸入文件 positive.review 和 negative.review，兩者都是XML檔。我們用BeautifulSoup讀進來，擷取review_text，然後用NLTK自建Tokenizer。 先產生 word-to-index map 再產生 word-frequency vectors。之後 shuffle data 創造 train/test splits，留100個給 test 用。接著用Logistic Regression 分類器找出訓練組和測試組的準確度(Accuracy)。接著我們可以看看每個單字的正負權重，可以訂一個閥值，比方絕對值大於正負0.5，以確認情緒是顯著的。最後我們找出根據現有演算法歸類錯誤最嚴重的正向情緒和負向情緒的例子。\n","\n","延伸:可用不同的tokenizer，不同的tokens_to_vector，不同的ML分類器做改進準確率的比較。最後可用您的model去預測unlabeled.review檔的內容。\n","\n","範例程式檔名: sentiment_情緒分析.py，以LogisticRegression 方式完成情緒分析。\n","\n","模組: sklearn, bs4, numpy, nltk\n","\n","輸入檔：stopwords.txt, /electronics 下 positive.review, negative.review\n","\n","成績：辨識百分率"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSy-f6om1MnC","executionInfo":{"status":"ok","timestamp":1630994422352,"user_tz":-480,"elapsed":373,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"5fec8b7f-8fae-4ac6-a21b-da11caa45498"},"source":["from __future__ import print_function, division\n","from future.utils import iteritems\n","from builtins import range\n","import nltk\n","import numpy as np\n","from sklearn.utils import shuffle\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.linear_model import LogisticRegression\n","from bs4 import BeautifulSoup\n","nltk.download(['punkt', 'wordnet'])"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"nrlSy3Xy1-zV"},"source":["wordnet_lemmatizer = WordNetLemmatizer()\n","\n","# from http://www.lextek.com/manuals/onix/stopwords1.html\n","stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n","\n","# 另一個 stopwords 的來源\n","# from nltk.corpus import stopwords\n","# stopwords.words('english')\n","\n","# 讀正向與負向 reviews\n","# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n","positive_reviews = BeautifulSoup(open('positive.review', encoding='utf-8').read(), features=\"html5lib\")\n","positive_reviews = positive_reviews.findAll('review_text')\n","\n","negative_reviews = BeautifulSoup(open('negative.review', encoding='utf-8').read(), features=\"html5lib\")\n","negative_reviews = negative_reviews.findAll('review_text')\n","\n","# 基於nltk自建 tokenizer\n","\n","def my_tokenizer(s):\n","    s = s.lower() # downcase\n","    tokens = nltk.tokenize.word_tokenize(s) # 將字串改為tokens\n","    tokens = [t for t in tokens if len(t) > 2] # 去除短字\n","    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # 去除大小寫\n","    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StWfOVA-4luv","executionInfo":{"status":"ok","timestamp":1630995487527,"user_tz":-480,"elapsed":3060,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"885ca929-286f-4985-8963-2bd811c3b55d"},"source":["# 先產生 word-to-index map 再產生 word-frequency vectors\n","# 同時儲存 tokenized 版本未來不需再做 tokenization\n","word_index_map = {}\n","current_index = 0\n","positive_tokenized = [] # store the tokenized text so that further tokenization is not needed\n","negative_tokenized = []\n","orig_reviews = []\n","\n","for review in positive_reviews:\n","    orig_reviews.append(review.text)\n","    tokens = my_tokenizer(review.text)\n","    positive_tokenized.append(tokens)\n","    for token in tokens:\n","        if token not in word_index_map:\n","            word_index_map[token] = current_index # build word-to-index map\n","            current_index += 1\n","\n","for review in negative_reviews:\n","    orig_reviews.append(review.text)\n","    tokens = my_tokenizer(review.text)\n","    negative_tokenized.append(tokens)\n","    for token in tokens:\n","        if token not in word_index_map:\n","            word_index_map[token] = current_index\n","            current_index += 1\n","\n","print(\"len(word_index_map):\", len(word_index_map))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["len(word_index_map): 11092\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqvmypcX4r7F","executionInfo":{"status":"ok","timestamp":1630995933453,"user_tz":-480,"elapsed":1194,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"d512b269-6fa7-48be-af1a-82882b6a8f3c"},"source":["# now let's create our input matrices\n","def tokens_to_vector(tokens, label):\n","    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\n","    for t in tokens:\n","        i = word_index_map[t]\n","        x[i] += 1\n","    x = x / x.sum() # 正規化數據提升未來準確度\n","    x[-1] = label\n","    return x\n","\n","N = len(positive_tokenized) + len(negative_tokenized)\n","# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\n","data = np.zeros((N, len(word_index_map) + 1))\n","i = 0\n","for tokens in positive_tokenized:\n","    xy = tokens_to_vector(tokens, 1)\n","    data[i,:] = xy\n","    i += 1\n","\n","for tokens in negative_tokenized:\n","    xy = tokens_to_vector(tokens, 0)\n","    data[i,:] = xy\n","    i += 1\n","\n","# shuffle data 創造 train/test splits\n","# 多次嘗試!\n","orig_reviews, data = shuffle(orig_reviews, data)\n","\n","X = data[:,:-1]\n","Y = data[:,-1]\n","\n","# 最後 100 列是測試用\n","Xtrain = X[:-100,]\n","Ytrain = Y[:-100,]\n","Xtest = X[-100:,]\n","Ytest = Y[-100:,]\n","\n","model = LogisticRegression()\n","model.fit(Xtrain, Ytrain)\n","print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n","print(\"Test accuracy:\", model.score(Xtest, Ytest))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy: 0.7826315789473685\n","Test accuracy: 0.68\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7d9F9oUL7EpK","executionInfo":{"status":"ok","timestamp":1630998412498,"user_tz":-480,"elapsed":315,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"9fb7db62-5bf1-4099-8ab1-a1d9aa1ac735"},"source":["# 列出每個字的正負 weight\n","# 用不同的 threshold values!\n","threshold = 0.5\n","for word, index in iteritems(word_index_map):\n","    weight = model.coef_[0][index]\n","    if weight > threshold or weight < -threshold:\n","        print(word, weight)\n","\n","\n","# 找出歸類錯誤的例子\n","preds = model.predict(X)\n","P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n","\n","# 只列出最糟的\n","minP_whenYis1 = 1\n","maxP_whenYis0 = 0\n","wrong_positive_review = None\n","wrong_negative_review = None\n","wrong_positive_prediction = None\n","wrong_negative_prediction = None\n","for i in range(N):\n","    p = P[i]\n","    y = Y[i]\n","    if y == 1 and p < 0.5:\n","        if p < minP_whenYis1:\n","            wrong_positive_review = orig_reviews[i]\n","            wrong_positive_prediction = preds[i]\n","            minP_whenYis1 = p\n","    elif y == 0 and p > 0.5:\n","        if p > maxP_whenYis0:\n","            wrong_negative_review = orig_reviews[i]\n","            wrong_negative_prediction = preds[i]\n","            maxP_whenYis0 = p\n","\n","print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n","print(wrong_positive_review)\n","print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n","print(wrong_negative_review)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["unit -0.7010595154469509\n","bad -0.7059793799785382\n","cable 0.7275943649758804\n","time -0.7262634923307855\n","'ve 0.7378994913992879\n","month -0.7405258833059363\n","sound 1.0903452364670843\n","lot 0.6391836696373069\n","you 1.0957396949954246\n","n't -1.9847789677553316\n","easy 1.8345738872346866\n","quality 1.4230501828143534\n","company -0.5024813614130383\n","card -0.5584481850471422\n","item -0.9003133512633421\n","wa -1.611789288996959\n","perfect 1.0162825197363154\n","fast 0.8757364397379382\n","ha 0.6723839077543783\n","price 2.9202296089006006\n","value 0.5649784626180324\n","money -1.1880277009286946\n","memory 0.9741276399883112\n","picture 0.5117812363423602\n","buy -0.9270604819792434\n","bit 0.6016819962353869\n","happy 0.5640104741180315\n","pretty 0.8111063902495742\n","doe -1.2535149858529508\n","highly 1.0261693014909732\n","recommend 0.73436739312428\n","customer -0.7557421732546674\n","support -0.8826361691141432\n","little 0.9006236825646172\n","returned -0.7335940612594839\n","excellent 1.32176679702361\n","love 1.2176271176117985\n","feature 0.5281932022766801\n","home 0.5212342115388671\n","useless -0.501238287917558\n","week -0.7622997195753834\n","size 0.5156967584335465\n","using 0.6303006517105494\n","video 0.5012397606185315\n","poor -0.7879212018947335\n","look 0.5688118049471148\n","then -1.0642006502413413\n","tried -0.8302617977264076\n","try -0.638687508431692\n","space 0.5810674478097578\n","comfortable 0.647510089001336\n","hour -0.545643187509109\n","expected 0.5037701422592047\n","speaker 0.9369272873873165\n","warranty -0.550797499903354\n","stopped -0.5440460280519589\n","junk -0.5459730112799962\n","paper 0.5548534517281214\n","terrible -0.5148160589132664\n","return -1.2526021715317321\n","waste -1.0079914063088973\n","refund -0.6248036862029187\n","Most wrong positive review (prob = 0.35785581419470236, pred = 0.0):\n","\n","I like the color and sony's technology of the earphone itself.\n","But the hanger that should be placed on your ear is worthless.\n","When I put the earphone in the ears, the hanger part is just hanging in the air, not supporting the earphone to be placed in and steady.\n","Because of that, the earphones easily pop out of your ear\n","\n","Most wrong negative review (prob = 0.6088175036102415, pred = 1.0):\n","\n","I bought this item rather than the cheaper products advertised because I trust Toshiba and because the picture quality was better than average.  Overall, I am very satisfied with the product.  We have used it on several aiplane trips and my toddler is very entertained.  We haven't had any problems with the mechanics of the player.  We also bought a case (which I highly recommend) so that we have enough Baby Einstein to keep her happy to our destination.  Also, the battery has never run out, and charges quickly.  My only problem with it is that the volume doesn't go up loud enough without earphones to hear it very well in a roaring commercial airliner.  But, my daughter mostly looks at the pictures anyway, so it wasn't that big a deal for us. \n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exlObuov_Uzy","executionInfo":{"status":"ok","timestamp":1630997432373,"user_tz":-480,"elapsed":385344,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"5771201c-3707-4e59-d300-db1606067933"},"source":["# classification of sentiment\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","\n","classifier_names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n","         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\"]\n","classifiers = [\n","    KNeighborsClassifier(3),\n","    SVC(kernel=\"linear\", C=0.025),\n","    SVC(gamma=2, C=1),\n","    DecisionTreeClassifier(max_depth=5),\n","    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n","    MLPClassifier(alpha=1, max_iter=1000),\n","    AdaBoostClassifier()]\n","\n","paired_c = zip(classifier_names, classifiers)\n","\n","for name_c, classifiers in paired_c:\n","  classifiers.fit(Xtrain,Ytrain)\n","  print(\"Classification method: {}, Train accuracy: {:.2f}, Test accuracy: {:.2f}\".format(name_c, classifiers.score(Xtrain,Ytrain), classifiers.score(Xtest, Ytest)))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification method: Nearest Neighbors, Train accuracy: 0.81, Test accuracy: 0.52\n","Classification method: Linear SVM, Train accuracy: 0.50, Test accuracy: 0.45\n","Classification method: RBF SVM, Train accuracy: 0.83, Test accuracy: 0.73\n","Classification method: Decision Tree, Train accuracy: 0.62, Test accuracy: 0.54\n","Classification method: Random Forest, Train accuracy: 0.56, Test accuracy: 0.53\n","Classification method: Neural Net, Train accuracy: 0.64, Test accuracy: 0.56\n","Classification method: AdaBoost, Train accuracy: 0.81, Test accuracy: 0.76\n"]}]}]}