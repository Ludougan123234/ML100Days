{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"cupoy_env","language":"python","name":"cupoy_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Day27_實作樹型模型_作業.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"pYv9IpWi89_O"},"source":["### 作業目的: 實作樹型模型\n","\n","在本次課程中實作了以Entropy計算訊息增益的決策樹模型，而計算訊息增益的方法除了Entropy只外還有Gini。因此本次作業希望讀者實作以Gini計算\n","\n","訊息增益，且基於課程的決策樹模型建構隨機森林模型。\n","\n","在作業資料夾中的`decision_tree_functions.py`檔案有在作業中實作的所有函式，在實作作業中可以充分利用已經寫好的函式"]},{"cell_type":"markdown","metadata":{"id":"SwHWv56F89_R"},"source":["### Q1: 使用Gini計算訊息增益\n","\n","$$\n","Gini = \\sum_{i=1}^cp(i)(1-p(i)) = 1 - \\sum_{i=1}^cp(i)^2\n","$$"]},{"cell_type":"code","metadata":{"id":"zrG-8Gjb89_S"},"source":["import pandas as pd\n","import numpy as np\n","from decision_tree_functions import decision_tree, train_test_split # shown below"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"9QGEdZEx89_T","executionInfo":{"status":"ok","timestamp":1630395584711,"user_tz":-480,"elapsed":287,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"94f4a4a9-f789-4082-ebcd-73c146b37f51"},"source":["# 使用與課程中相同的假資料\n","\n","training_data = [\n","    ['Green', 3.1, 'Apple'],\n","    ['Red', 3.2, 'Apple'],\n","    ['Red', 1.2, 'Grape'],\n","    ['Red', 1, 'Grape'],\n","    ['Yellow', 3.3, 'Lemon'],\n","    ['Yellow', 3.1, 'Lemon'],\n","    ['Green', 3, 'Apple'],\n","    ['Red', 1.1, 'Grape'],\n","    ['Yellow', 3, 'Lemon'],\n","    ['Red', 1.2, 'Grape'],\n","]\n","\n","header = [\"color\", \"diameter\", \"label\"]\n","\n","df = pd.DataFrame(data=training_data, columns=header)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>color</th>\n","      <th>diameter</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Green</td>\n","      <td>3.1</td>\n","      <td>Apple</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Red</td>\n","      <td>3.2</td>\n","      <td>Apple</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Red</td>\n","      <td>1.2</td>\n","      <td>Grape</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Red</td>\n","      <td>1.0</td>\n","      <td>Grape</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Yellow</td>\n","      <td>3.3</td>\n","      <td>Lemon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    color  diameter  label\n","0   Green       3.1  Apple\n","1     Red       3.2  Apple\n","2     Red       1.2  Grape\n","3     Red       1.0  Grape\n","4  Yellow       3.3  Lemon"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"dlb0Wwmw89_U"},"source":["#Gini impurity\n","def calculate_gini(data):\n","    \n","    #取的資料的label訊息\n","    ##<your code>####\n","    label_data = data[:, -1]\n","    \n","    #取得所有輸入資料的獨立類別與其個數\n","    ###<your code>###\n","    _, counts = np.unique(label_data, return_index=True) \n","    # this returns two things: the sorted unique value and indices of where the unique value first appears\n","\n","    #計算機率\n","    ###<your code>###\n","    prob = counts / counts.sum() \n","\n","    #計算gini impurity\n","    ###<your code>###\n","    gini = 1 - sum(pow(prob, 2))\n","\n","    return gini"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlXCdNF589_V","executionInfo":{"status":"ok","timestamp":1630397628712,"user_tz":-480,"elapsed":287,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"8b87c52f-a6bc-4851-b855-b0c76393efda"},"source":["#分割資料集\n","###<your code>###\n","train_df, test_df = train_test_split(df, test_size = 0.1)\n","\n","#以Gini inpurity作為metric_function訓練決策樹\n","tree = decision_tree(calculate_gini, 'classification', counter = 0, min_samples=2, max_depth=5)\n","tree.fit(train_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'color = Green': ['Apple', {'color = Red': ['Grape', 'Lemon']}]}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4sfOSY189_W","executionInfo":{"status":"ok","timestamp":1630398700143,"user_tz":-480,"elapsed":261,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"bbecf633-51dc-4c3c-93a2-f4b886868f95"},"source":["# 以建構好的樹進行預測\n","sample = test_df.iloc[0]\n","###<your code>###\n","prediction = tree.pred(sample, tree.sub_tree)\n","\n","print(\"===Sample=== \\n{} \\n===Results of prediction===\\n{}\".format(sample, prediction))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["===Sample=== \n","color         Red\n","diameter        1\n","label       Grape\n","Name: 3, dtype: object \n","===Results of prediction===\n","Grape\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zri_Uw9f89_X"},"source":["### Q2: 實作隨機森林\n","利用決策樹來實作隨機森林模型，讀者可參考隨機森林課程講義。\n","\n","此份作業只要求讀者實作隨機sample訓練資料，而隨機sample特徵進行訓練的部分，讀者可以參考`decision_tree_functions.py`中的`get_potential_splits`與`decision_tree`部分(新增參數`random_features`)"]},{"cell_type":"code","metadata":{"id":"IjnW4Wrg89_Y"},"source":["class random_forest():\n","    '''Random forest model\n","    Parameters\n","    ----------\n","    n_boostrap: int\n","        number of samples to sample to train indivisual decision tree\n","    n_tree: int\n","        number of trees to form a forest\n","    '''\n","    \n","    def __init__(self, n_bootstrap, n_trees, task_type, min_samples, max_depth, metric_function, n_features=None, random_features = None):\n","        self.n_bootstrap = n_bootstrap\n","        self.n_trees = n_trees\n","        self.task_type = task_type\n","        self.min_samples = min_samples\n","        self.max_depth = max_depth\n","        self.metric_function = metric_function\n","        self.n_features = n_features\n","        self.random_features = random_features\n","    \n","    def bootstrapping(self, train_df, n_bootstrap):\n","        #sample data to be used to train individual tree\n","        ###<your code>###\n","        df_bootstrapped = train_df.sample(n_bootstrap)\n","\n","        #avoid picking the samples with all the same label\n","        while len(np.unique(df_bootstrapped.label)) == 1:\n","          df_bootstrapped = train_df.sample(n_bootstrap)\n","              \n","        return df_bootstrapped\n","    \n","    def fit(self, train_df):\n","        \n","        self.forest = []\n","        \n","        for x in range(self.n_trees):\n","          df_bootstrapped = self.bootstrapping(train_df, self.n_bootstrap)\n","          tree = decision_tree(self.metric_function,\n","                               task_type = self.task_type,\n","                               counter = 0,\n","                               min_samples = self.min_samples,\n","                               max_depth = self.max_depth,\n","                               random_features = self.random_features)\n","          tree.fit(df_bootstrapped)\n","          self.forest.append(tree)\n","\n","        return self.forest\n","    \n","    def pred(self, test_df):\n","        df_predictions = {}\n","        \n","        ###<your code>###\n","        for i,y in enumerate(self.forest):\n","          df_predictions[f'tree_{i}'] = test_df.apply(lambda x : tree.pred(x, tree = tree.sub_tree), axis = 1)\n","        df_predictions = pd.DataFrame(df_predictions)\n","        \n","        #majority voting\n","        ###<your code>###\n","        random_forest_predictions = df_predictions.mode(axis = 1)\n","\n","        return random_forest_predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPfA-84M89_Y","outputId":"06cd4c72-7877-4893-eb46-78f7a107e608"},"source":["train_df, test_df = train_test_split(df, 0.2)\n","\n","#建立隨機森林模型\n","forst = random_forest(train_df)\n","\n","forest.fit(train_df)"],"execution_count":null,"outputs":[{"data":{"text/plain":["[<decision_tree_functions.decision_tree at 0x10f829f28>,\n"," <decision_tree_functions.decision_tree at 0x10f8297f0>,\n"," <decision_tree_functions.decision_tree at 0x10f8261d0>,\n"," <decision_tree_functions.decision_tree at 0x10f82ba20>]"]},"execution_count":299,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_ESFoBchVg0","executionInfo":{"status":"ok","timestamp":1630402144252,"user_tz":-480,"elapsed":274,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"1e63318d-38fe-4ead-b0ec-b97a00603c9b"},"source":["train_df, test_df = train_test_split(df, 0.2)\n","#建立隨機森林模型\n","forest = random_forest(n_bootstrap=2, n_trees=4, task_type='classification',min_samples = 2, max_depth = 5, metric_function=calculate_gini)\n","forest.fit(train_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[<__main__.decision_tree at 0x7f65ffc22c10>,\n"," <__main__.decision_tree at 0x7f65ffc22290>,\n"," <__main__.decision_tree at 0x7f65ffbf8b90>,\n"," <__main__.decision_tree at 0x7f65ffc22ad0>]"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LC-oEhGhiQ-L","executionInfo":{"status":"ok","timestamp":1630402231741,"user_tz":-480,"elapsed":259,"user":{"displayName":"潘明然Randy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHP39ZMKPSdxXqImi_guYL7WEKYknrhIbQSC7cYA=s64","userId":"01531977523423804940"}},"outputId":"c4911219-7fb2-46cb-c64f-d03e0c26da66"},"source":["test_DAT = test_df\n","prediction = forest.pred(test_df)\n","\n","print(\"+++test data+++\\n{}\\n+++prediction+++\\n{}\".format(test_DAT, prediction))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+++test data+++\n","    color  diameter  label\n","7     Red       1.1  Grape\n","5  Yellow       3.1  Lemon\n","+++prediction+++\n","       0\n","7  Grape\n","5  Lemon\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yVrFBTWnjId0"},"source":["#### The code below is taken from Decision_tree_functions.py provided by Cupoy"]},{"cell_type":"code","metadata":{"id":"B7V9s3ubIkWe"},"source":["import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pprint import pprint\n","\n","# 分割測試集與訓練集  \n","def train_test_split(df, test_size=0.1):\n","    \n","    if isinstance(test_size, float):\n","        test_size = round(test_size * len(df))\n","\n","    #以隨機的方式取的測試集資料點的index\n","    indices = list(df.index)\n","    test_indices = random.sample(population=indices, k=test_size)\n","\n","    #分割測試集與訓練集\n","    test_df = df.loc[test_indices]\n","    train_df = df.drop(test_indices)\n","    \n","    return train_df, test_df\n","\n","\n","# 檢查資料是否都為同一類別\n","def check_purity(data):\n","    '''Function to check if input data all belong to the same class\n","    Parameter\n","    ---------\n","    data: list\n","        Input data\n","    '''\n","    #取的資料的label訊息\n","    labels = data[:, -1]\n","    \n","    #檢查是否所有的label都為同一種\n","    unique_classes = np.unique(labels)\n","    \n","    if len(unique_classes) == 1:\n","        return True\n","    else:\n","        return False\n","    \n","# 根據給定的資料，取得每個特徵(feature)可能做為樹型模型分割節點的值\n","# 可能作為分割節點得值即為每個特徵的獨特值(unique value)\n","def get_potential_splits(data, random_features=None):\n","    '''Function to get all potential split value for tree base model\n","    Parameter\n","    ---------\n","    data: list\n","        Input data\n","    '''\n","    \n","    potential_splits = {}\n","    _, n_columns = data.shape\n","    column_indices = list(range(n_columns - 1)) #此處的-1是為了扣掉label的欄位\n","    \n","    if random_features and random_features <= len(column_indices):\n","        #隨機選取特徵進行訓練\n","        column_indices = random.sample(population=column_indices, k=random_features)\n","    \n","    for column_index in column_indices:    \n","        \n","        #根據欄位取的特徵的獨特值(unique values)\n","        values = data[:, column_index]\n","        unique_values = np.unique(values)\n","        \n","        #將取得的可能分割值除存在potential_split的字典中(key=特徵欄位的index, value:此特徵可能的分割值)\n","        potential_splits[column_index] = unique_values\n","    \n","    return potential_splits\n","\n","\n","#由給定的輸入DataFrame給個特徵值的型態(數值型特徵或類別型特徵)\n","def determine_type_of_feature(df):\n","    '''Function to get features types\n","    Parameter\n","    ---------\n","    df: pd.DataFrame\n","        Input raw pd.DataFrame data\n","    '''\n","    \n","    feature_types = []\n","    \n","    #若特徵的獨特值個數較少，及當作類別型特徵資料(若為數值型，獨特值個數應該會很多)\n","    #此處簡易的將判斷方法設為資料個數的1/3次方，此值可以自行修改選較為適合的個數\n","    n_unique_values_treshold = int(len(df)**(1/3))\n","    \n","    for feature in df.columns:\n","        if feature != \"label\":\n","            unique_values = df[feature].unique()\n","            rep_value = unique_values[0] #選出一個值做此特徵的代表\n","\n","            if (isinstance(rep_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n","                feature_types.append(\"categorical\")\n","            else:\n","                feature_types.append(\"continuous\")\n","    \n","    return feature_types\n","\n","\n","# 根據給定的資料、欲採用特徵欄位指標(index)與欲採用的分割值，來取的分割節點分割後的左節點資料與右節點資料\n","def split_data(data, split_column, split_value):\n","    '''Function to splitted left and right nodes\n","    Parameter\n","    ---------\n","    data: list\n","        Input data\n","    split_column: int\n","        index for feature column\n","    split_value: float or int or string\n","        value to be used as split benchmark\n","    '''\n","    \n","    #取得用來分割的特徵欄位\n","    split_column_values = data[:, split_column]\n","\n","    #依據欄位值的型態(數值型特徵或類別型特徵)來進行節點分割\n","    type_of_feature = FEATURE_TYPES[split_column]\n","    \n","    if type_of_feature == \"continuous\":\n","        #數值型特徵分割\n","        data_left = data[split_column_values <= split_value]\n","        data_right = data[split_column_values >  split_value]\n","    else:\n","        #類別型特徵分割\n","        data_left = data[split_column_values == split_value]\n","        data_right = data[split_column_values != split_value]\n","    \n","    return data_left, data_right\n","\n","\n","# 根據給定的資料與任務類型(回歸或分類)來產生終端節點\n","def create_leaf(data, task_type):\n","    '''Function to create leaf node\n","    Parameters\n","    ----------\n","    data: list\n","        Input data\n","    task_type: str\n","        indicate the type of tree (regression or classification)\n","    '''\n","    \n","    #取的資料的label欄位\n","    label_column = data[:, -1]\n","    \n","    if task_type == \"regression\":\n","        #回歸任務\n","        leaf = np.mean(label_column)\n","    else:\n","        #分類任務\n","        #取得所有輸入資料的獨立類別與其個數\n","        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n","        \n","        #以個數最多的類別，作為此節點的輸出類別\n","        index = counts_unique_classes.argmax()\n","        leaf = unique_classes[index]\n","    \n","    return leaf\n","\n","#計算資料的熵(Entropy)\n","def calculate_entropy(data):\n","    \n","    #取的資料的label訊息\n","    label_column = data[:, -1]\n","    \n","    #取得所有輸入資料的獨立類別與其個數\n","    _, counts = np.unique(label_column, return_counts=True)\n","\n","    #計算機率\n","    probabilities = counts / counts.sum()\n","    \n","    #計算entropy\n","    entropy = sum(probabilities * -np.log2(probabilities))\n","     \n","    return entropy\n","\n","\n","#取得左節點與右節點訊息合\n","def calculate_overall_metric(data_below, data_above, metric_function):\n","    \n","    n = len(data_below) + len(data_above)\n","    p_data_below = len(data_below) / n\n","    p_data_above = len(data_above) / n\n","\n","    overall_metric =  (p_data_below * metric_function(data_below) \n","                     + p_data_above * metric_function(data_above))\n","    \n","    return overall_metric\n","\n","\n","#以迴圈的方式計算所有可能分割值的訊息增益，取的最佳的分割特徵與值(訊息增益最大)\n","def determine_best_split(data, potential_splits, metric_function, task_type='classification'):\n","    \n","    #紀錄是否為樹的第一層(第一次回圈)\n","    first_iteration = True\n","    \n","    for column_index in potential_splits:\n","        for value in potential_splits[column_index]:\n","            \n","            #根據給定的特徵與分割值分割資料為左節點、右節點\n","            data_left, data_right = split_data(data, split_column=column_index, split_value=value)\n","            \n","            #判斷是回歸樹亦或分類樹\n","            if task_type == \"regression\":\n","                #回歸樹\n","                current_overall_metric = calculate_overall_metric(data_left, data_right, metric_function=metric_function)\n","            else:\n","                #分類樹\n","                current_overall_metric = calculate_overall_metric(data_left, data_right, metric_function=metric_function)\n","\n","            if first_iteration or current_overall_metric <= best_overall_metric:\n","                first_iteration = False\n","                \n","                best_overall_metric = current_overall_metric\n","                best_split_column = column_index\n","                best_split_value = value\n","    \n","    return best_split_column, best_split_value\n","\n","\n","class decision_tree():\n","    '''Decision Tree model\n","    Parameters\n","    -----------\n","    metric_function: function\n","        the metric function used to calculate information gain\n","    task_type: str\n","        indicate the type of tree (regression or classification)\n","    counter: int\n","        counter for recording number of splits\n","    min_samples: int\n","        minimum number of samples for a node to be able to split\n","    max_depth: int\n","        Maximum depth for the decision tree\n","    '''\n","    def __init__(self, metric_function, task_type='classification', counter=0, min_samples=2, max_depth=5, random_features=None):\n","        \n","        self.metric_function = metric_function\n","        self.task_type = task_type\n","        self.counter = counter\n","        self.min_samples = min_samples\n","        self.max_depth = max_depth\n","        self.random_features = random_features\n","    \n","    def fit(self, df):\n","        '''\n","        df: pd.DataFrame\n","            input raw DataFrame data\n","        '''\n","        # 資料準備\n","        if self.counter == 0:\n","            #若為第一次分割，取出資料特徵的欄位與其對應的型態\n","            global COLUMN_HEADERS, FEATURE_TYPES\n","\n","            #取得資料特徵欄位\n","            COLUMN_HEADERS = df.columns\n","            #取的特徵型態\n","            FEATURE_TYPES = determine_type_of_feature(df)\n","            #取得資料特徵值\n","            data = df.values\n","        else:\n","            #取得資料特徵值\n","            data = df           \n","\n","        # 終端節點處理(leaf)\n","        # 若資料都屬於同一種類別、資料個數小於最小可分割個數、樹的深度大於最大深度，節點即屬於終端節點(leaf)\n","        if (check_purity(data)) or (len(data) < self.min_samples) or (self.counter == self.max_depth):\n","            leaf = create_leaf(data, self.task_type)\n","            return leaf\n","\n","        # 分割節點\n","        else:    \n","            self.counter += 1\n","\n","            # 節點分割的左節點與右節點\n","            potential_splits = get_potential_splits(data, self.random_features)\n","            split_column, split_value = determine_best_split(data, potential_splits,\n","                                                             self.metric_function, self.task_type)\n","            data_left, data_right = split_data(data, split_column, split_value)\n","\n","            # 若分割後的左節點或右節點sample個數為零(代表母節點即無法在分割)\n","            if len(data_left) == 0 or len(data_right) == 0:\n","                # 取出此節點\n","                leaf = create_leaf(data, self.task_type)\n","                return leaf\n","\n","            # 取得分割節點的分割依據(特徵與分切值)\n","            feature_name = COLUMN_HEADERS[split_column]\n","            type_of_feature = FEATURE_TYPES[split_column]\n","\n","            if type_of_feature == \"continuous\":\n","                #連續型數值\n","                question = \"{} <= {}\".format(feature_name, split_value)\n","            else:\n","                #類別型數值\n","                question = \"{} = {}\".format(feature_name, split_value)\n","\n","            # 建構子樹(sub-tree)\n","            sub_tree = {question: []}\n","\n","            # 已遞迴的方式取建構完整決策樹    \n","            yes_answer = self.fit(data_left)\n","            no_answer = self.fit(data_right)\n","\n","            #若左節點與右節點分割的結果相同，則此節點及不需再進行分割\n","            #此情形會發生在此節點資料個數小於min_samples或樹深度大於max_depth\n","            if yes_answer == no_answer:\n","                sub_tree = yes_answer\n","            else:\n","                sub_tree[question].append(yes_answer)\n","                sub_tree[question].append(no_answer)\n","            \n","            self.sub_tree = sub_tree\n","            \n","            return self.sub_tree\n","        \n","    def pred(self, example, tree):\n","        # 使用訓練好的決策樹進行預測\n","        \n","        #取得分割節點(由上到下)\n","        question = list(tree.keys())[0]\n","        feature_name, comparison_operator, value = question.split(\" \")\n","\n","        #以節點分割問題分類資料\n","        if comparison_operator == \"<=\":\n","            #數值型資料\n","            if example[feature_name] <= float(value):\n","                answer = tree[question][0]\n","            else:\n","                answer = tree[question][1]\n","        else:\n","            #類別型資料\n","            if str(example[feature_name]) == value:\n","                answer = tree[question][0]\n","            else:\n","                answer = tree[question][1]\n","        \n","        # 若分類完成，返回分類結果\n","        if not isinstance(answer, dict):\n","            return answer\n","        else:\n","            #繼續往下分類\n","            residual_tree = answer\n","            return self.pred(example, residual_tree)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG0Dv3Lh9_y6"},"source":[""],"execution_count":null,"outputs":[]}]}