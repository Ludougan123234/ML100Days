{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day18_Individual assignment.ipynb","provenance":[],"authorship_tag":"ABX9TyPp7e1/h3Q1HDooKVxSA57D"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PzyZeOA6kaY6"},"source":["# Supervised learning vs. Unsupervised learning \n","In supervised learning, the algorithms learn on (a) labeled dataset(s), while in unsupervised learning, the algorithms try to make sense of the data on its own.\n","## Supervised learning: \n","(Algorithms: SVM, Naive Bayes, Decision tree, linear/logistic regression, random forests, etc) \n","### Advantages:\n","1. Better control over the training dataset (contents of the datasets and the ways that the data should be classified) \n","2. After training, the dataset can be discarded from memory\n","3. Very helpful in classification problems & numeric predictions \n","\n","### Disadvantages: \n","1. Bad for handling data inputs that are complex/not in the training dataset\n","2. Always labeling the data for the algorithm is unrealistic. Algorithms will eventually have to learn to label the data on their own. \n","3. Have  to monitor training data diversity for accurate outputs \n","\n","---\n","\n","##Unsupervised learning: \n","(Algorithms: PCA, K-means, Locally-linear embedding, t-distributed Stochastic Neighbor Embedding (t-SNE), etc) \n","###Advantages: \n","1.  Faster build speed/Less complexity (no labeling required) \n","2.  Ease of data acquisition\n","3.  Real-time\n","\n","###Disadvantages: \n","1. No artificial inteference (to fine tune the classification process), could be damaging to the classification criteria\n","2. Less reliable/accurate\n","\n","---\n","# PCA and LDA \n","Both PCA (Principle Component Analysis) and LDA (Linear Discriminant Analysis) are algorithms for dimensionality reduction. They both reduce the number of random variables with respect to principle variables that are present in the dataset. \n","\n","## PCA\n","Use PCA when you 1) Want to educe the number of variables when you are not able to identify variables to be removed. 2) Want to ensure the independence between variables 3) are comfortable making the variables less interpretable. \n","\n","###Steps of conducting PCA: \n","1. Standardize the column/Recenter the dataset according to the mean value of the dataset  (the recentered dataset will have a mean value of 0) \n","2. Compute variance-covariance matrix C\n","3. Compute Eigenvalues (magnitude of variance) and Eigenvectors (direction of the principle component) \n","4. Take the dot product of the Eigenvalue and columns to get the principle component features. \n","\n","\n","##LDA\n","This algorithm also helps with dimentionality reduction, but focuses more on the characteristics between the data that gives the best discrimination to seperate the data. (To find a line to seperate the data) \n","\n","###Steps of conducting LDA: \n","\n","\n","\n","\n","---\n","\n","References:\n","1. NVidia. [\"SuperVize Me: What’s the Difference Between Supervised, Unsupervised, Semi-Supervised and Reinforcement Learning?\"](https://blogs.nvidia.com/blog/2018/08/02/supervised-unsupervised-learning/#:~:text=In%20a%20supervised%20learning%20model,and%20patterns%20on%20its%20own.)\n","2. Towards Data Science. [\"Types of Machine Learning Algorithms You Should Know\"](https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861)\n","3. Analytics Vidhya. [\"PCA vs LDA vs T-SNE — Let’s Understand the difference between them!](https://medium.com/analytics-vidhya/pca-vs-lda-vs-t-sne-lets-understand-the-difference-between-them-22fa6b9be9d0)\n","\n","\n"]}]}