{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Day22_手刻貝氏-作業.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"A2IWd05A6xtH"},"source":["# 手刻基本Naive Bayes模型"]},{"cell_type":"markdown","metadata":{"id":"kGzT8T2G6xtK"},"source":["#### 學習重點：理解單純貝氏模型原理"]},{"cell_type":"markdown","metadata":{"id":"nhfBe1RE6xtL"},"source":["---"]},{"cell_type":"code","metadata":{"id":"WQLj9kQg6xtM"},"source":["import re\n","import numpy as np\n","import math\n","import os\n","import glob\n","import codecs\n","\n","from collections import defaultdict\n","from collections import Counter\n","\n","def tokenize(message):\n","    message=message.lower()\n","    all_words=re.findall(\"[a-z0-9]+\",message)\n","    return set(all_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Xcc8VH96xtN"},"source":["### 讀入資料並分割為 train/testset"]},{"cell_type":"code","metadata":{"id":"m5NNhtt16xtO"},"source":["X = []\n","Y = []\n","paths =[r'spam_data/spam', r'spam_data/easy_ham', r'spam_data/hard_ham'] \n","for path in paths:\n","    for fn in glob.glob(path+\"/*\"):\n","        if \"ham\" not in fn:\n","            is_spam = True\n","        else:\n","            is_spam = False\n","        #codecs.open可以避開錯誤，用errors='ignore'\n","        with codecs.open(fn,encoding='utf-8', errors='ignore') as file:\n","            for line in file:\n","                #這個line的開頭為Subject:\n","                if line.startswith(\"Subject:\"):\n","                    subject=re.sub(r\"^Subject:\",\"\",line).strip()\n","                    X.append(subject)\n","                    Y.append(is_spam)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IF765O7n6xtP"},"source":["from sklearn.model_selection import train_test_split\n","# random_state 是為了讓各為學員得到相同的結果，平時可以移除\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CI4BuATj6xtQ"},"source":["train_data = []\n","test_data = []\n","\n","for x_, y_ in zip(X_train, y_train):\n","    train_data.append([x_, y_])\n","\n","for x_, y_ in zip(X_test, y_test):\n","    test_data.append([x_, y_])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWwUsMiC6xtR"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"fhRxxpej6xtR"},"source":["### defaultdict用法示範"]},{"cell_type":"code","metadata":{"id":"B4sxf1nw6xtR","outputId":"6d00adf8-f186-4ba5-c252-a56f932451a8"},"source":["from collections import defaultdict\n","\n","mess='This is our first time in Taiwan,,,,, such a beautiful contury'\n","\n","counts=defaultdict(lambda:[0,0])\n","counts['you'][0]+=1\n","counts['hi'][0]+=1\n","counts['hi'][1]+=2\n","counts['no'][1]+=1\n","counts['no'][0]+=8\n","print('dic : {}'.format(counts))\n","print('you : {}'.format(counts['you']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dic : defaultdict(<function <lambda> at 0x7f8d595de200>, {'you': [1, 0], 'hi': [1, 2], 'no': [8, 1]})\n","you : [1, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rz6JWPS56xtT"},"source":["### 創造一個字典，裡面是{'hi': [1, 0]}，對應第一個數字是是垃圾郵件的次數，對應第二個數字是不是垃圾郵件的次數"]},{"cell_type":"code","metadata":{"id":"AgvTPNba6xtT"},"source":["def count_words(training_set):\n","    counts=defaultdict(lambda:[0,0])\n","    for message,is_spam in training_set:\n","        for word in tokenize(message):\n","            counts[word][0 if is_spam else 1] += 1\n","            # if word is 'is_spam', counts[word][0] += 1\n","            # if word is not 'is_spam', counts[word][1] += 1\n","            '''自行填入， list[0]為出現在spam中的次數，list[1]為出現在ham(非spam)中的次數'''\n","    return counts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6bS73jh16xtT"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"6civ9emP6xtU"},"source":["## 計算 p(w|spam) / p(w|non_spam)\n","* 其中K為超參數，為了確保分母/分子皆不為0"]},{"cell_type":"code","metadata":{"id":"aJm50ycm6xtU"},"source":["def word_probabilities(counts,total_spams,total_non_spams,k=0.5):\n","    #獲得三組數據，分別為w這個字，p(w|spam)，p(w|non_spam)\n","    #counts[w][0]=spam,counts[w][1]=non_spam\n","    return [(w,\n","             (counts[w][0]+k)/(total_spams +2*k),\n","             (counts[w][1]+k)/(total_non_spams+2*k)) for w in counts]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2CYZ3Q66xtU"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"kZwXLwmS6xtU"},"source":["## 計算貝氏結果"]},{"cell_type":"code","metadata":{"id":"0KR9aNIa6xtV"},"source":["def spam_probability(word_probs,message, is_spam_probability, is_not_spam_probability):\n","    #先把這個mail的文字處理一下\n","    message_words=tokenize(message)\n","    #初始化值\n","    log_prob_if_spam=log_prob_if_not_spam=0.0\n","    #將w這個字，p(w|spam)，p(w|non_spam)依序引入\n","    #taking the log of the probabilities to avoid underflow error\n","    for word,prob_if_spam,prob_if_not_spam in word_probs:\n","        if word in message_words:\n","            #假如這個字有在這個mail中出現\n","            #把他的p(w|spam)轉log值加上log_prob_if_spam\n","            log_prob_if_spam= log_prob_if_spam + math.log(prob_if_spam)\n","            #把他的p(w|non_spam)轉log值加上log_prob_if_not_spam\n","            log_prob_if_not_spam= log_prob_if_not_spam + math.log(prob_if_not_spam)\n","        else:\n","            #如果沒出現log_prob_if_spam+上得值就是1-p(w|spam)也就是這封信是垃圾郵件但是w這個字卻沒在裡面\n","            # a bit unclear to me (I only filled in parts of the code) as to why there is an else section\n","            # considering the words that are in the dictionary but not in the sentence. \n","            # Update: (according to an expert)\n","            # p(spam|sentence) ∝ p(spam)* p(words in a sentence|spam)* (1-p(words in the dictionary|spam)) \n","            # there are two things to consider when seeing a sentence with the \"spam\" label:\n","            # 1. Given that the sentence is a spam, the words are present in the sentence\n","            # 2. Given that the sentence is a spam, we do not see other words from the dictionary\n","            #    (and thus, the probability values of the words that are in the dictionary\n","            #    but not in the sentence have to be considered as well (????))\n","            log_prob_if_spam=log_prob_if_spam+math.log(1-prob_if_spam)\n","            log_prob_if_not_spam=log_prob_if_not_spam+math.log(1-prob_if_not_spam)\n","    log_prob_if_spam = log_prob_if_spam + math.log(is_spam_probability)\n","    log_prob_if_not_spam = log_prob_if_not_spam + math.log(is_not_spam_probability)\n","    \n","    #把+起來的值轉成exp再算NaiveBayes\n","    prob_if_spam= math.exp(log_prob_if_spam)\n","    prob_if_not_spam= math.exp(log_prob_if_not_spam)\n","    #貝氏\n","    return prob_if_spam/(prob_if_spam+prob_if_not_spam)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPT0T-Ej6xtV"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"5t86aytc6xtV"},"source":["### 打包整個模型"]},{"cell_type":"code","metadata":{"id":"c7ZGIpa86xtV"},"source":["class NaiveBayesClassifier:\n","    def __init__(self,k=0.5):\n","        self.k=k\n","        self.word_probs=[]\n","    def train(self,training_set):\n","        #訓練的資料格式為(message,is_spam)\n","        #所有垃圾郵件的數量\n","        num_spams=len([is_spam for message,is_spam in training_set if is_spam])\n","        #所有不是垃圾郵件的數量\n","        num_non_spams=len(training_set)-num_spams\n","        \n","        self.is_spam_probability = num_spams / len(training_set)\n","        self.is_not_spam_probability = num_non_spams / len(training_set)\n","        #把training_set裡面的所有字體轉成('Bad',num_is_spam,num_not_spam)\n","        word_counts=count_words(training_set)\n","        self.word_probs=word_probabilities(word_counts,num_spams,num_non_spams,self.k)\n","    def classify(self,message):\n","        return spam_probability(self.word_probs,message, self.is_spam_probability, self.is_not_spam_probability)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yga8qhTC6xtW"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Yhu0Q_bZ6xtW"},"source":["### Fit 訓練集"]},{"cell_type":"code","metadata":{"id":"37wlYix66xtW"},"source":["classifier=NaiveBayesClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tu5QSZhz6xtX"},"source":["classifier.train(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ECxQlZL6xtX"},"source":["### 預測"]},{"cell_type":"code","metadata":{"id":"KMAZAYNI6xtX"},"source":["classified=[(subject,is_spam,classifier.classify(subject)) for subject,is_spam in test_data]\n","counts=Counter((is_spam,spam_probability>0.5) for _,is_spam,spam_probability in classified)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbTDDofL6xtX","outputId":"b566438b-2190-4803-c635-6d6189abfee3"},"source":["# |                    | actually positive | actually negative |\n","# |--------------------|-------------------|-------------------|\n","# | predicted positive | TP                | FP                |\n","# | predicted negative | FN                | TN                |\n","#\n","# ACCURACY = (TP+FP)/(TP+FP+FN+TN)\n","# PRECISION = TP / (TP+FP)\n","# RECALL = TP / (TP + FN)\n","\n","counts"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({(False, False): 578,\n","         (True, True): 57,\n","         (False, True): 9,\n","         (True, False): 41})"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"QMfQIQth6xtX","outputId":"b42b3c5f-da03-4d2f-aa80-a03d6848b996"},"source":["precision=counts[(True, True)]/(counts[(True, True)]+counts[(False, True)])\n","recall=counts[(True, True)]/(counts[(True, True)]+counts[(True, False)])\n","binary_accuracy = (counts[(True, True)]+counts[(False, False)])/(counts[(False, True)]+counts[(False, False)]+counts[(True, True)]+counts[(True, False)])\n","print('accuracy : {:.2f}%'.format(binary_accuracy*100))\n","print('precision : {:.2f}%'.format(precision*100))\n","print('recall : {:.2f}%'.format(recall*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy : 92.70%\n","precision : 86.36%\n","recall : 58.16%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4bmpO_9Fd5dG"},"source":["# further references:\n","# 1. https://medium.com/%E5%B1%95%E9%96%8B%E6%95%B8%E6%93%9A%E4%BA%BA%E7%94%9F/python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-110-%E5%96%AE%E7%B4%94%E8%B2%9D%E6%B0%8F%E5%88%86%E9%A1%9E%E5%99%A8-50cdd9ce16f7\n"],"execution_count":null,"outputs":[]}]}