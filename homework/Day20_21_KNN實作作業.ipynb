{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day20_21-KNN實作作業.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S"
      },
      "source": [
        "# K-Nearest Neighbors (K-NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE7efpb1Pgly"
      },
      "source": [
        "### 參考課程實作並在datasets_483_982_spam.csv的資料集中獲得90% 以上的 accuracy (testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import codecs\n",
        "import re"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "hF-6ev8QPgl8",
        "outputId": "50d4c02b-a6fd-4040-de0f-cfb1b7c81d50"
      },
      "source": [
        "dataset = pd.read_csv(r'datasets_483_982_spam.csv', encoding = 'latin-1')\n",
        "dataset"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        v1  ... Unnamed: 4\n",
              "0      ham  ...        NaN\n",
              "1      ham  ...        NaN\n",
              "2     spam  ...        NaN\n",
              "3      ham  ...        NaN\n",
              "4      ham  ...        NaN\n",
              "...    ...  ...        ...\n",
              "5567  spam  ...        NaN\n",
              "5568   ham  ...        NaN\n",
              "5569   ham  ...        NaN\n",
              "5570   ham  ...        NaN\n",
              "5571   ham  ...        NaN\n",
              "\n",
              "[5572 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "bdILzMagyj8w",
        "outputId": "a47b76b6-808e-4195-b8ee-6beddf26c1ac"
      },
      "source": [
        "dataset = dataset[[\"v1\", \"v2\"]] #remove unnecesary and empty columns \n",
        "dataset.columns = [\"ham_or_spam\", \"text\"] #rename columns\n",
        "dataset #view dataset"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ham_or_spam</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     ham_or_spam                                               text\n",
              "0            ham  Go until jurong point, crazy.. Available only ...\n",
              "1            ham                      Ok lar... Joking wif u oni...\n",
              "2           spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3            ham  U dun say so early hor... U c already then say...\n",
              "4            ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...          ...                                                ...\n",
              "5567        spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568         ham              Will Ì_ b going to esplanade fr home?\n",
              "5569         ham  Pity, * was in mood for that. So...any other s...\n",
              "5570         ham  The guy did some bitching but I acted like i'd...\n",
              "5571         ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQn3mFW0U49o"
      },
      "source": [
        "all_data = []\n",
        "for items in dataset.itertuples():\n",
        "  #print(items[1])\n",
        "  #print(items[2])\n",
        "  text = items[2]\n",
        "  if items[1] == \"ham\":\n",
        "    spam = 0\n",
        "  else:\n",
        "    spam = 1\n",
        "  all_data.append([text, spam])"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsXnDm_P2i6K"
      },
      "source": [
        "all_data = np.array(all_data)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZtE2TDlPgl-"
      },
      "source": [
        "### 取出訓練內文與標註"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-59Vcj-rPgl_"
      },
      "source": [
        "X = all_data[:,0]\n",
        "Y = all_data[:,1].astype(np.uint8)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpbE2OZcPgmB",
        "outputId": "7183c09b-f6ae-4000-f9a5-7315252bef84"
      },
      "source": [
        "print('Training Data Examples : \\n{}'.format(X[:5]))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data Examples : \n",
            "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
            " 'Ok lar... Joking wif u oni...'\n",
            " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
            " 'U dun say so early hor... U c already then say...'\n",
            " \"Nah I don't think he goes to usf, he lives around here though\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v89vC_JPgmD",
        "outputId": "44dd1f8d-ec87-49ff-e5e2-f82ac1758b6a"
      },
      "source": [
        "print('Labeling Data Examples : \\n{}'.format(Y[:5]))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labeling Data Examples : \n",
            "[0 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypSwHYQ5PgmD"
      },
      "source": [
        "### 文字預處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZssxLi-5Z8V",
        "outputId": "229a8b19-18c4-482f-d665-ef24aa950629"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMHFN-v3PgmF"
      },
      "source": [
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "  #print(nltk.pos_tag([word])) output format \"nice\" = [('nice', 'JJ')])\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper\n",
        "  #[0][1][0] = first character in the POS string\n",
        "  tag_dict = {\n",
        "      \"J\":wordnet.ADJ,\n",
        "      \"N\":wordnet.NOUN,\n",
        "      \"V\":wordnet.VERB,\n",
        "      \"R\":wordnet.ADV\n",
        "  }\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def clean_content(content):\n",
        "  content_clean = [re.sub(\"[^a-zA-Z]\", \" \", x).lower() for x in content] #remove non alphabetic letters\n",
        "  content_tokenize = [nltk.word_tokenize(words) for words in content_clean]  #tokenize\n",
        "  content_stopword_lemmatizer = []\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  for word in content_tokenize:\n",
        "    word_clean = []\n",
        "    for w in word:\n",
        "      if w not in stop_words:\n",
        "        w = lem.lemmatize(w, get_wordnet_pos(w))\n",
        "        word_clean.append(w)\n",
        "    content_stopword_lemmatizer.append(word_clean)\n",
        "  output = [' '.join(x) for x in content_stopword_lemmatizer]\n",
        "  return output"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgVwLiyPgmG"
      },
      "source": [
        "X = clean_content(X)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAXEdk1kPgmH"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "GKhJEtMQPgmJ",
        "outputId": "b2aec7d6-9d07-41ca-ad0b-fe5cb350753f"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#max_features是要建造幾個column，會按造字出現的高低去篩選 \n",
        "cv=CountVectorizer(max_features = 2000)\n",
        "X=cv.fit_transform(X).toarray()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-88b7078bdc38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#max_features是要建造幾個column，會按造字出現的高低去篩選\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCzZDnrCPgmK",
        "outputId": "e3e5f6b9-8f9f-4f32-b672-76220bb3b33c"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxIPVyMhmKp"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzJWAXIhxoC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6jCOCQiAmP"
      },
      "source": [
        "## Training the K-NN model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0pFVAmciHQs",
        "outputId": "d8969ba5-ac57-47eb-ee31-7b0ec0b6d383"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyxW5b395mR2"
      },
      "source": [
        "## Predicting a new result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8YOXsQy58rP",
        "outputId": "cb3fa66b-275a-4b55-9cef-4fe5415c6c47"
      },
      "source": [
        "print('Trainset Accuracy: {}'.format(classifier.score(X_train, y_train)))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainset Accuracy: 0.9409916984518735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6_cPNp5PgmQ",
        "outputId": "35117346-9173-46a4-d1d9-ec17cef5ef77"
      },
      "source": [
        "print('Testset Accuracy: {}'.format(classifier.score(X_test, y_test)))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testset Accuracy: 0.9139013452914798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYVQH-l5NpE"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6VMTb2O4hwM"
      },
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Hwj34ziWQW"
      },
      "source": [
        "## Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6bpZwUiiXic",
        "outputId": "bae5dd05-b217-453a-d3be-21138f9503c0"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[949   0]\n",
            " [ 96  70]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9139013452914798"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oSxfiV_OjK7"
      },
      "source": [
        "##### oh don't mind me i'm just playing with some parameters here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUKl4s3UF21L",
        "outputId": "ddc00b1c-8d69-434e-990b-0fe0e9209e99"
      },
      "source": [
        "features = [500, 1000, 1500, 2000, 2500] #trying if different features would impact training scores\n",
        "for num_feat in features:\n",
        "  dataset = pd.read_csv(r'datasets_483_982_spam.csv', encoding = 'latin-1')\n",
        "  dataset = dataset[[\"v1\", \"v2\"]] #remove unnecesary and empty columns \n",
        "  dataset.columns = [\"ham_or_spam\", \"text\"] #rename columns\n",
        "  all_data = []\n",
        "  for items in dataset.itertuples():\n",
        "    text = items[2]\n",
        "    if items[1] == \"ham\":\n",
        "      spam = 0\n",
        "    else:\n",
        "      spam = 1\n",
        "    all_data.append([text, spam])\n",
        "  all_data = np.array(all_data)\n",
        "  X = all_data[:,0]\n",
        "  Y = all_data[:,1].astype(np.uint8)\n",
        "  X = clean_content(X)\n",
        "  print(\"now trying {} features\".format(num_feat))\n",
        "  cv=CountVectorizer(num_feat)\n",
        "  X=cv.fit_transform(X).toarray()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
        "  classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  print('Trainset Accuracy: {}'.format(classifier.score(X_train, y_train)))\n",
        "  print('Testset Accuracy: {}'.format(classifier.score(X_test, y_test)))\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(cm)\n",
        "  accuracy_score(y_test, y_pred)\n",
        "  # *surprised pikachu face*"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "now trying 500 features\n",
            "Trainset Accuracy: 0.9365043751402289\n",
            "Testset Accuracy: 0.9085201793721973\n",
            "[[949   0]\n",
            " [102  64]]\n",
            "now trying 1000 features\n",
            "Trainset Accuracy: 0.9365043751402289\n",
            "Testset Accuracy: 0.9085201793721973\n",
            "[[949   0]\n",
            " [102  64]]\n",
            "now trying 1500 features\n",
            "Trainset Accuracy: 0.9365043751402289\n",
            "Testset Accuracy: 0.9085201793721973\n",
            "[[949   0]\n",
            " [102  64]]\n",
            "now trying 2000 features\n",
            "Trainset Accuracy: 0.9365043751402289\n",
            "Testset Accuracy: 0.9085201793721973\n",
            "[[949   0]\n",
            " [102  64]]\n",
            "now trying 2500 features\n",
            "Trainset Accuracy: 0.9365043751402289\n",
            "Testset Accuracy: 0.9085201793721973\n",
            "[[949   0]\n",
            " [102  64]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6kYKwlzPzFb",
        "outputId": "b93c070e-aa99-409c-f6c0-7fc7b78a2722"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "n_neighbors = [5, 10, 25, 50, 100, 200, 500, 1000] ## 可自行嘗試不同K值\n",
        "for k in n_neighbors:\n",
        "\tclassifier = KNeighborsClassifier(n_neighbors = k, metric = 'minkowski', p = 2)\n",
        "\t# cv = 10 代表切成10等分\n",
        "\taccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,n_jobs=-1)\n",
        "\t\n",
        "\tprint('設置K值:{}'.format(k))\n",
        "\tprint('Average Accuracy: {}'.format(accuracies.mean()))\n",
        "\tprint('Accuracy STD: {}'.format(accuracies.std()))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "設置K值:5\n",
            "Average Accuracy: 0.919002871970575\n",
            "Accuracy STD: 0.0072759624394765415\n",
            "設置K值:10\n",
            "Average Accuracy: 0.8907346198417898\n",
            "Accuracy STD: 0.006333371713162194\n",
            "設置K值:25\n",
            "Average Accuracy: 0.8712132816042727\n",
            "Accuracy STD: 0.0020847926038850264\n",
            "設置K值:50\n",
            "Average Accuracy: 0.8696432710233285\n",
            "Accuracy STD: 0.0006565610639949747\n",
            "設置K值:100\n",
            "Average Accuracy: 0.8696432710233285\n",
            "Accuracy STD: 0.0006565610639949747\n",
            "設置K值:200\n",
            "Average Accuracy: 0.8696432710233285\n",
            "Accuracy STD: 0.0006565610639949747\n",
            "設置K值:500\n",
            "Average Accuracy: 0.8696432710233285\n",
            "Accuracy STD: 0.0006565610639949747\n",
            "設置K值:1000\n",
            "Average Accuracy: 0.8696432710233285\n",
            "Accuracy STD: 0.0006565610639949747\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}